{"/":{"title":"WaterStop","content":"\nHello! My name is Alex, and I'm a software engineer. On [itch.io](https://waterstop.itch.io/) I put\nmy playable projects\n\n# Site content \n\n[ReGIna WIP](regina/regina) - documentation about WIP programming language\n\n[100 days](100-days/100-writing-days) of English essays\n\n[other notes](notes)\n","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/100-writing-days":{"title":"100 writing days","content":"\nPreviously I did 100 reading days, but looking back, I think It was mostly about\nachieving consistency rather than improving pronunciation. Additionally, it is much easier than\nwriting, because it's consuming, not creating. This time, I'll try to write every day for 100 days,\nfollowing these rules:\n\n1. Write on paper and scan it with Google Lens. Handwriting is more beneficial for improving\n   writing ability.\n2. Write at least 10 sentences and 300 words.\n\nLinks to all articles:\n\n28 June: [Heap and Stack](100-days/heap-and-stack)\n\n29 June: [How languages are run?](100-days/transpilers-compilers-interpreters)\n\n30 June: [Abstract syntax tree](100-days/ast)\n\n1 July: [C#: boxing, unboxing and equality operator](100-days/boxing-and-equality-cs)\n\n2 July: [Introduction to grammars](100-days/introduction-to-grammars)\n\n3 July:\n\n1. [Parsers](100-days/parsers),\n2. added CST definition to [AST post](100-days/ast) and moved\n   parser classification to parsers,\n3. added non-formal grammars section to [grammars post](100-days/introduction-to-grammars),\n\n4 July: [Devlog 1. Bad globals](100-days/devlog1)\n\n5 July: [Devlog 2. Link class. Removing state](100-days/devlog2)\n\n6 July: [Shallow traits of a villain](100-days/shallow-villain-traits). Update a page to view.\nSimilarly, update on return\n\n7 July: [Multithreading. Primitive tools, definitions](100-days/multithreading1)\n\n8 July: [Multithreading. Problems](100-days/multithreading2)\n\n9 July: [Quality of life](100-days/quality-of-life)\n\n10 July: [Why game making is not for me](100-days/why-gamemaking-is-not-for-me)\n\n11 July: Day off \u003e_\u003c\n\n12 July: after further consideration, I decided to take about 2 weeks off. ","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/ast":{"title":"Abstract Syntax Tree","content":"\nAbstract Syntax Tree (AST) is key concept in program evaluation. It is a tree with\ntokens as its nodes. Tree is a better structure than sequence. Here is why:\n\n1. Operator precedence.\n2. Hierarchy\n\n### Operator precedence\n\nConsider a following expression : $5 + 7 \\cdot 2 + 3$. It is obvious to us humans which arithmetic\noperator. we should calculate first. But it is not that simple for a machine.\nHence, we introduce a tree structure:\n\n{{\u003c svg \"static/ast_arithmetic.svg\" \u003e}}\n\nNow we need to recursively evaluate each child before calculating the result of the overall\nexpression.\n\n### Hierarchy\n\nLet's look at the following python code:\n\n```python\ndef some_method():\n    return 1\n\n\nclass SomeClass:\n    field = 1\n    other = \"other\"\n\n    def class_method(self):\n        return self\n\n    def other_one(self):\n        return \"not classMethod\"\n```\n\nHere is a sketch of AST representation:\n\n```\nsome_method:\n    type: function\n    children:\n        params: ...\n        body: ...\nSomeClass:\n    type: class\n    children:\n        field:\n            type: variable\n            children:\n                value: ...\n        other:\n            type: variable\n            children:\n                value: ...\n        class_method:\n            type: function\n            children:\n                params: ...\n                body: ...\n         other_one:\n            type: function\n            children:\n                params: ...\n                body: ...\n        \n```\n\nNotice how all the functions of class are on the same level. We could even add `fields` and\n`functions` and `classes` nodes to separate statements of different kinds:\n\n```\nfunctions:\n    some_method:\n        ...\nclasses:\n    SomeClass:\n        type: class\n        children:\n            fields:\n                field:\n                    ...\n                other:\n                   ...\n            functions:\n                class_method:\n                    ...\n                 other_one:\n                    ...\n```\n\nThis hierarchical structure is predictable and manageable.\n\n### Node structure\n\nAST trees do not have a commonly accepted structure. It's content varies depending on\nusage. But I found some general structure of AST node. Nodes usually include.\n\n* node type (declaration of class, method, variable, infix operator, block etc)\n* node value: e.g. name of a particular identifier\n* children: nodes that a re children of the current one\n* meta information: position in the initial file, path to initial file...\n\n# Creating AST\n\nA pipeline consists of two steps:\n\n`Code (text) -\u003e Tokens -\u003e CST`[^1]` -\u003e AST`\n\nTo create tokens from code, a tokenizer (aka Lexer) is used. Usually it's pretty straightforward:\nread until next whitespace, identify obtained token.\n\nTo create AST from tokens, we need a parser. I won't go into much detail here, because parsers are\ncomplicated enough for a separate [post](100-days/parsers). \n\n[^1]: Some parsers create an intermediate representation of AST called concrete syntax tree. The\ndifference between AST and CST is that each AST node has a semantic purpose, while CST may contain\ndetrimental tokens that are used to make parsing of a grammar work. Example of CST generated probably\nwith a parser based on EBNF arithmetic [grammar](100-days/introduction-to-grammars):\n![cst.png](100-days/images/cst.png)\n`Expr`, `Term` and `Factor` nodes do not have a semantic purpose, therefore technically it is not\nan AST.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/boxing-and-equality-cs":{"title":"C#: Boxing, unboxing and equality operator","content":"\n# Boxing / unboxing\n\nBoxing/unboxing has to do with reference and value types. Value types store their values on\nstack (usually primitives). Reference types store value in heap and reference to that value on\nstack.\n\nBoxing is process of converting value type to reference type, unboxing is\nthe opposite. These procedures are computationally expensive, because:\n\n* to box a value, a new object should be created.\n* to unbox a value, it should be cast to a particular type, which is time-consuming (albeit to a\n  lesser extent than boxing). To avoid boxing, one\n  should use generics.\n\n# Equality\n\nThere are two types of equality: reference and value.\nReference equality is applicable only to reference types.\nIt checks that two objects point to the same value in heap.\nValue equality checks that two variables contain the same value.\n\n*Note: `==` operator will call `ReferenceEquals` for reference types and is not applicable for two\nvariables of different type.*\n\n### Examples\n\n```C#\n  int a = 3;\n  object pA = a; // implicit boxing\n  object explicitPA = (object)a; // explicit boxing\n  int b = (int)pA; // explicit unboxing - cast an object to type (the only possible way)\n  int implicitUnboxing = pA; // Error: Cannot implicitly convert type 'object' to 'int'\n\n  Console.WriteLine(ReferenceEquals(a, pA)); // False\n  Console.WriteLine(Equals(a, pA)); // True\n  Console.WriteLine(ReferenceEquals((object)a, pA)); // False\n  Console.WriteLine(a == b); // True, calls Equals()\n  Console.WriteLine((object)a == (object)b); // False, calls ReferenceEquals()\n  Console.WriteLine(a == pA); // Error: Operator '==' cannot be applied to operands if type 'int' and 'object'\n```","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/cpp-pointers":{"title":"C++ Memory model","content":"\nMemory is a two column table. First row contains **addresses**, second row contains **values**.\n\n```C++\nint x = 4;\n```\n\nWhen variable is assigned, an empty memory row cell is chosen. Variable's value is written to\nsecond column of the row.\n|Address|Value|Assigned variable (not part of a memory, just a showcase example)|\n|------|------| ---- |\n|0x1000| 4| x|\n|0x1004| 0x1000|pX|\n|0x1008|4|y|\n\n# Pointer\n\n```C++\nint* pX = \u0026x;\n```\n\nVariable declaration with a `*` after type states that it is a pointer to that type. In this\ncase, **pX is an integer pointer**. Instead of storing value, pointer variable store memory address\nin their value column (second column of a memory row).\n\n`\u0026` is for extracting address from a variable. `\u0026x` means **address of variable named x**\n\n# Dereference\n\n```C++\nint y = *pX;\n```\n\nVariable with a preceding asterisk (`*`) referred to as a **dereference**. When used before a\npointer variable, this expression will return a value in a row, to which such variable points to.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/devlog1":{"title":"Devlog 1. Bad globals","content":"\n### What is a symbol table?\n\nSymbol table stores current program state. I separated it into three levels,\nturning `SymbolTable` into a kind of manager-facade:\n\n1. **FileTable**. Contains classes, objects and functions in current file.\n2. **variableTable**. Contains functions and properties of current\n   instance (if `ClassA.someClassFunction()` is resolved, instance of ClassA is a variableTable in\n   current `SymbolTable`). Variable table is not a special class, it's either a primitive or a\n   class instance.\n3. **ScopeTable**. Contains all variables and arguments for current function.\n\n### Introduction\n\nI created global SymbolTable `globalTable` and global `imports`. It was convenient at the time.\nAll classes, functions and objects are added to the `globalTable` during semantic analysis.\nSuperclasses are defined, imports too. Semantic analysis is performed for initial file and all\nits dependencies (they are files). It is handy to write imports and file contents to global\nvariables. But when I started writing tests, it backfired.\n\n### Running tests\n\nWhen running a program once, evaluation is done once. However, when running tests, evaluation runs\nmany times. It lead to all kinds of issues:\n\n1. Functions appeared twice, especially mains, because main in current file and main from previous\n   failed session were saved.\n2. Phantom functions were called from unclean imports\n\nTo fix that, I added `clear()` function to make global Table a blank table and delete all imports.\nIt is called at the end of `evaluate()` and `eval()` (which is a light version of `evaluate()`).\nBut it was treating a symptom, not dealing with a problem. I plan to make it possible to run\nmultiple threads in parallel. So I need to remove these globals. Probably I can create an array of\nglobal tables, each thread will write to a separate table, although\nI would like to make stateless code, not add another crutch.\n\nThen I discovered same problems with tests that check failure behavior. Particularly check whether\nexceptions are thrown correctly. It appeared they\ndid, however a clean program state wasn't restored after. Exception was thrown, but `globalTable` and\n`imports` weren't cleared after. I added `clear()` on `PositionalException` initialization.\n\n### Future work\n\nAdd `imports` to `FileTable` as a property. Remove `globalTable` and pass it as an argument.\n","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/devlog2":{"title":"Devlog 2. Link class. Removing state","content":"\nLink class is for evaluating links. A link is a statement in form of `a.b.c`, where all expressions\nafter dots (in this case `b` and `c`) are either identifiers[^1], indexes[^2] or invocations[^3].\nExpression before the dot is anything, even the ternary operator.\nThese expressions are link children.\n\n### Link evaluation\n\nLinks are resolved iteratively, starting from the first expression. Every evaluated expression\nreturns a property (a type or a primitive). But the first child might be a name of an imported\nfile, therefore it is resolved differently from all the next children.\n\nDuring dynamic creation of a class it is important to know what parts of the link are\nassigned. For example:\n\n```kotlin\nclass Start {\n    a = mid.end.endProperty // a is not assigned until mid, end and endProperty are\n    mid = Middle()\n    mid.end.addedEndProperty = \"anotherEndValue\" /** it is applicable for\n        left hand-side of assignment too. mid and end need to be assigned first\n        **/\n}\nclass Middle {\n    end = End()\n}\nclass End {\n    endProperty = \"endValue\"\n}\n```\n\n### Adding state, encountering problems\n\nFor convenience, I added these properties to the link instance:\n\n* `index`: index of currently resolved child\n* `currentVariable`: value of currently resolved child\n* `currentParent`: previously resolved child\n* `table`: symbolTable with `currentVariable` as `variableTable`\n* `initialTable`: symbolTable before link\n  evaluation, used to resolve function arguments. It is useful in the following code:\n\n```kotlin\nclass A {\n    a = 0\n    b = B()\n    fun aFunction() {\n        b.bFun(a, b) /** here table will change variableTable to B instance, therefore a and b\n            properties will not be found for bFun call. That's why invocation arguments are resolved\n            using initialTable\n            **/\n    }\n}\nclass B {\n    fun bFun(arg1,arg2) {\n        // do something\n    }\n}\n```\n\nThese variables are cleared after the end of the evaluation. However, I didn't think about\nrecursive functions:\n\n```kotlin\nclass A {\n    a = if(iter \u003c 5) A() else Nothing()\n    iter = if(parent == 0) 0 else parent.iter /** parent is a special property.\n        It returns an instance from which this instance is created or 0 **/\n    fun str() {return \"iter, \" + a.str()}\n}\n\nclass Nothing() {\n    fun str() { return \"end\" }\n}\n```\n\nIn that case, when calling `str()`, variable values weren't cleared. I might've come up with some\nhack, but again, I knew that was bad\nsolution. Link represents a token, it is not a place to store evaluation values. So I moved state\nto function arguments. Link is still a poorly written class that has bugs. I work on fixing them\nand refactoring Link.\n\n[^1]: Identifier is a word, starting with a letter. All other symbols are letters, underscores or\nnumbers\n[^2]: Index is a token for getting a value from string, array or dictionary. It is an identifier\nwith square brackets. Square brackets contain index (or a key, if identifier is a name of a\ndictionary). `arrayName[2]`, `dictionaryName[\"key\"]`\n[^3]: Invocation is a function call or a class constructor. `call(a+b, someArgument=value)`\n, `ClassA(propertyName=value)`","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/heap-and-stack":{"title":"Basic memory management","content":"\n*Last fall, I had an interview.\nThey asked me about differences between heap and stack, how\nreferences and variables are stored.\nI was confident that I knew all this stuff.\nApparently, I was wrong.*\n\n# Commonalities\n\nBoth stack and heap use RAM.\n\n# Stack\n\nStack is a LIFO static[^1] storage that contains:\n\n* Local primitive variables\n* References to non-primitive variables\n* Method parameters\n* Sequence of method calls (hence, each thread has its own stack. Also, this is the reason why\n  infinite recursion will result in stack overflow error)\n\nWhen the method finishes its execution, a stack is emptied with one assembly command - moving\nthe pointer to stack head. Therefore, a stack returns to the state before that method's call.\n\n### Primitive variables\n\nLocal primitive variables are stored in stack. Primitive fields of an object instance are stored in\nheap.\n\n# Heap\n\nHeap is a dynamic memory structure that stores non-primitive variables. References (pointers to\nvariable address in the heap) to these variables are in the stack.\n\nUnlike stack, heap is commonly shared between threads. Consequently, one object shouldn't be\nmodified simultaneously from different threads.\n\nOut of memory error is possible for the heap too.\n\n### Garbage collector and C/C++ features\n\nSome languages (Java, C#) handle memory automatically. A component responsible for it is called\ngarbage collector. Fallback of this approach is unexpected runtime slowdowns due to garbage\ncollector managing memory.\n\nOther languages (C/C++) pass this responsibility on to programmer. In C++ every `new` keyword\nshould\nbe followed by `delete` to prevent memory leak. Additionally, it is possible to store object\ninstance on stack in C++, if variable is declared without `new`. Such local variable will\nbe popped from stack after method\nexecution. [A little more about C++ memory model](100-days/cpp-pointers).\n\n### Heap fragmentation\n\nHeap is a single block of memory. When some memory from heap is released, a chunk of unused memory\nis formed. After many iterations heap becomes a bunch of alternating fragments of used and unused\nmemory.\n\nEssentially this is an inefficient memory utilization, because overall there might be $n$\nbytes of free memory, however not all bytes can be allocated. It is possible to allocate no more\nbytes than there are in the biggest consecutive chunk of free memory.\n\n{{\u003c svg \"static/barcode.svg\" \u003e}}\n\n*After some time heap looks like a barcode (white is free memory, black is unused memory)*\n\n[^1]: some languages allow stack reallocation","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/introduction-to-grammars":{"title":"Introduction to grammars","content":"\nGrammar defines a set of strings (words), which is called a **language**.\nEssentially grammar is a `Dictionary \u003cString, String\u003e`, where each dictionary entry is a\n**grammar rule**.\n\nOther two important definitions are terminal and non-terminal symbols.\nTerminal is a string that cannot be resolved into something else,\nunlike non-terminal. Consequently, a result of grammar resolve should always be a sequence\nof terminals.\n\nFor terminals a lowercase letter is assigned, for non-terminal a capital one. Also, there is a\nspecial starting non-terminal marked `S`\n(we assume that we start creating words starting with `S`).\n\n### Creating a grammatically correct word\n\nStarting with `S`, on each step we choose a non-terminal symbol and a rule with the same symbol on\nthe left-hand side. Then such non-terminal is replaced with the right-hand side of the rule. Until\nthere is a non-terminal, a process is continued.\n\nConsider a following example of a grammar:\n\n```\ngrammar gA\nS -\u003e a\nS -\u003e aS\n```\n\nTo create a word `aaa`:\n\n1. Have `S`, use `S -\u003e aS`, now have `aS`\n2. Use `S -\u003e aS`, result is `aaS`\n3. Use `S -\u003e a`, end with `aaa`. All non-terminals are removed.\n\nGenerally this grammar describes a language of words, containing any number of `a`s:\n$L(gA) = \\\\{a^n|n\u003e0\\\\}$\n\n### The Chomsky hierarchy\n\nGrammar with a smaller type number contains all grammars with a bigger type number.\n\n#### Context free grammar (type 2)\n\nGrammars with a single non-terminal symbol on the left of each rule are called context free\ngrammars (CFG). Because no context is required for replacing a non-terminal.\n\n#### Regular grammar (type 3)\n\nSubset of content free grammars is regular grammars.\nIt puts a restriction on the right-hand side of\nits rules, three types of expressions are allowed:\n\n* empty string\n* single terminal\n* terminal followed by non-terminal.\n\nExample of a regular grammar:\n\n```\ngrammar gR\nS -\u003e aX\nS -\u003e EOW // (end of word)\nX -\u003e bS\n```\n\nThis grammar defines a language $L(gR) = \\\\{(ab)^n|n\\geqslant0\\\\}$\n\nContext free grammar is a pushdown automata, regular grammar is a finite state machine.\n{{\u003c svg \"static/finite_state_regular.svg\" \u003e}}\n\n#### Context-sensitive grammar (type 1)\n\nContext-sensitive grammars allow left-hand side to contain context, which must be similar in the\nright-hand side. Formally speaking, each rule of context-sensitive grammar is:\n$$c_0Ac_1 \\rightarrow c_0bc_1$$ where:\n\n* $c_0,\\\\; c_1$ are sequences of terminals and non-terminals\n  (sequence might be empty),\n* $A$ is a non-terminal\n* $b$ is a non-empty sequence of terminals and non-terminals.\n\n#### Unrestricted grammar (type 0)\n\nUnrestricted grammars are grammars without any restrictions to their rules.\n\n### BNF, EBNF\n\nThere is a special Backus-Naur form and Extended Backus-Naur form (BNF, EBNF) to\ndescribe a context free grammar. This notation is used most commonly. Here is\na [python grammar defined with EBNF and PEG](https://docs.python.org/3/reference/grammar.html).\n\n# Non formal grammars\n\nBefore going any further, let's bring in some clarifications.\n\n### Grammar notation\n\nPreviously we used capitals for non-terminals and lowercase for terminals. But in real grammars\nidentifiers are used. We consider every identifier that appears on the left hand-side a\nnon-terminal (we work with grammars having only a non-terminal on the left-hand side),\nother identifiers are terminals.\n\nIntroduce new symbols:\n\n* `|` - choice operator (or). Grammar rule that contains `|` is actually multiple grammar rules:\n  `S -\u003e a | b | ... | n` is\n\n```\nS -\u003e a\nS -\u003e b\n...\nS -\u003e n\n```\n\nIf the rule is more complicated, containing nested choice operators, it is rewritten in a similar\nmanner:\n\n```\nS -\u003e (a | b) '+' d | c\n// rewritten to\nS -\u003e a '+' d \nS -\u003e b '+' d\nS -\u003e c\n```\n\n* `:` in EBNF colon replaces arrow to divide two sides of a rule. I'll stick to the arrow for\n  the time, to keep the style consistent and justify the use of code blocks\n  (I use FiraCode with ligatures in code blocks to make arrow a continuous symbol)\n\nAll grammars discussed before are formal grammars. Formal grammars have terminals and non-terminals\nin their rules and no other operators of special symbols.\n\n### Parsing expression grammars or PEGs\n\nPEGs are very similar to CFGs, but they are not **ambiguous**,\nmeaning that for any input only one AST can be generated. It is achieved by selecting the first\nmatch in a choice operator. For CFG[^1] each variant in a choice operator is equal in terms of\nprecedence (and because of that two ASTs may be generated from one input).\n\n### Recursive grammars\n\nif there is a rule with a non-terminal on the left side, which can be derived again by applying\nsome rules to the resulting expression.\n\n```\ngrammar rG\nA -\u003e aBC\nB -\u003e Db\nC -\u003e c\nD -\u003e Ad\n```\n\nrG is recursive, because:\n\n1. starting with `A -\u003e aBC`\n2. apply `B -\u003e Db` to `aBC`, get `aDbC`\n3. apply `D -\u003e Ad` to `aDbC` get `aAddC`. Non-terminal `A` is derived again.\n   Proved, that rG is recursive.\n\nAlmost all non-trivial grammars are recursive.\n\n#### Left recursion\n\nLeft recursive grammar is a grammar that contains a recursive rule, deriving form which will\neventually create the same non-terminal as the first symbol.\n\n```\ngrammar lrG\nA -\u003e BC\nB -\u003e Db\nC -\u003e c\nD -\u003e Ad\n```\n\nlrG is left recursive, because same sequence of rules as in rG will produce AddC. A is the first\nsymbol, therefore lrG is left recursive.\n\nSome parsers are unable to parse left recursive grammars, descending into infinite recursion.\nFor that reason, a grammar is rewritten, making it less humanly readable.\nAny sensible left-recursive grammar can be rewritten, I think.\nThis grammar is not reasonable: `A -\u003e A`.\n\n[^1]: When introducing CFG, I stated that it is a part of a Chomsky hierarchy, hence it is a formal\ngrammar. Consequently, only terminals and non-terminals are allowed in the rules.\nBut CFG can be mentioned outside the formal grammar set, keeping its distinctive characteristic of\none\nnon-terminal on the left-hand side. Therefore, CFG right-hand side can contain other special\nsymbols. From now on, if not explicitly specified, I'll use CFG term implying that **it is not a\nformal grammar**.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/multithreading1":{"title":"Multithreading. Primitive tools, definitions","content":"\nFirst, let's clarify some definitions (I won't go too deep into that):\n\n### Process\n\n**Process** is a running program, that has its own address space in memory (most of it is heap).\n**Multithreading** is running multiple programs in OS simultaneously (like browser, messenger, IDE,\ntask\nmanager...)\n\n### Thread\n\n**Thread** is an execution unit of a process, it uses process heap but has its own stack.\n\n**Parallelism** or **multithreading** is multiple threads running at once.\n\n**Concurrency** means managing multiple threads of execution, not necessarily at the same time.\nParallelism is a method of concurrency (I'm not sure about that one, I took it from\n[here](https://stackoverflow.com/questions/4844637/what-is-the-difference-between-concurrency-parallelism-and-asynchronous-methods#comment5379841_4844774))\n.\n\n### Asynchronous and synchronous\n\nSynchronous blocks of code execute after each other, **next one cannot be executed before previous\nis not finished**. Imperative language statements are synchronous (code runs line by line).\nAsynchronous blocks of code, which are also called **tasks** might run simultaneously, their run\nindependently[^1] form each other.\n\n### Basic multithreading tools in Java\n\nJava has a `Thread` class and a `Runnable` interface. Both of them have a `run()` method, which\nbody\nis executed asynchronously to the main thread[^2], when `start()` is invoked. Example of starting\nRunnable and Thread asynchronously:\n\n```java\npublic static void main(String[] args) {\n    Runnable ir = ImplementedRunnable();\n    Thread t = Thread(ir); // to run a runnable, instantiate a wrapper-thread\n    Thread dt = DerivedThread();\n    t.start();\n    dt.start();\n    // result of this code might be \"thread runnable \" or \"runnable thread \", it is not determined.\n}\n\nclass DerivedThread extends Thread {\n    public void run() {\n        System.out.print(\"thread \");\n    }\n}\n\nclass ImplementedRunnable implements Runnable {\n    public void run() {\n        System.out.print(\"runnable \");\n    }\n}\n```\n\n#### Useful thread methods\n\nThread's execution might be interrupted with `Thread.interrupt()`.\n\n`Thread.join()` will make a current thread wait until a joined thread instance finished its\nexecution.\n\n```java\n\nvoid joinExample() {\n  Thread dt = DerivedThread();\n  dt.start();\n  dt.join();\n  System.out.print(\"2\");\n  // calling joinExample() always prints \"thread 2\"\n}\n```\n\n### Synchronized block/method\n\nSynchronized blocks can be run only by one thread at a time. This is done by creating a **\nmonitor** (aka **lock**), which is some object that a running thread takes. Only a thread with a\nmonitor A can access a synchronized block, that is synchronized on monitor A.\n\nSynchronized method is implicitly locked on the instance that its being invoked on. Static\nsynchronized methods lock on the class itself.\n\n### Atomic\n\nAtomic actions are performed momentarily and cannot be interrupted. They either complete or not run\nat all. Atomic operations:\n\n* read/write references and primitive values, excluding `long` and `double`\n* read/write variables with `volatile` keyword.\n  There are special classes, like `AtomicInteger` that make all its main operations (addition,\n  multiplication...) atomic.\n\n[^1]: well, if they use synchronous methods, then some of their runtime is dependent.\n[^2]: when a program is started, a main thread is created. It is a user thread. Main difference\nbetween daemon and user threads is that a program won't finish until there is a running user\nthread. Daemon threads are service threads, for instance garbage collection is a daemon thread.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/multithreading2":{"title":"Multithreading. Problems, Object methods","content":"\n### Thread problems\n\nThread problems have names. Here are the most popular ones.\n\n#### Deadlock\n\nN locks locked forever, waiting for each other. For instance, two monitors are required to move\nfurther, and two threads have one monitor taken. They both will be waiting until another monitor is\nfreed, which won't happen. In Java there are no tools to identify deadlock.\n\n#### Livelock\n\nThreads interact with each other not performing any useful work.\n\n#### Starvation\n\nThread waits for monitor access for too long, while other threads access that monitor multiple\ntimes.\n\n### Useful synchronization Object methods\n\nAll following methods require a thread to have a monitor. Hence, these methods can be used only\nin `synchronized` block/method.\n\n**wait()**. When reaching this command, thread frees a monitor and is put into a waiting\nlist, pausing its execution. Usually `wait()` is surrounded in a `while` cycle.\n\n**notify()**. Frees one random thread waiting for a particular monitor and gives them a monitor.\n\n**notifyAll()**. Empties a waiting list of threads waiting. A random thread gets a monitor, while\nall other threads go back to waiting list. The only difference that I found out with `notify()` is\nthat with `notify()` JVM selects a thread, with `notifyAll()` the system thread scheduler does it.\nProbably system thread scheduler guarantees that all waiting threads will, in some time, acquire\nlock, while JVM does not (it might accidentally pick a subset of waiting threads all the time,\nputting other threads to starvation).\n\n#### Spurious wakeups\n\nSometimes waiting threads can get out of a waiting list. This happens due to specific thread\nimplementations in different OS. Also thread can be woken from an `interrupt()` call. That is one\nreason why `wait()` should be surrounded in a `while` block - to put spontaneously woken thread\nback into waiting.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/parsers":{"title":"Parsers","content":"*As with the first article, I wrote this one on a computer, because the topic is hard to tackle. I\nknew I would have to make a lot of edits and rearrangements. So technically I'm two days behind my\nschedule, but we're just going to ignore that.*\n\n# Parser traits\n\nThese traits are not exactly definitive features for a parser. However, these definitions are often\nmentioned, so it's good to know what they mean.\n\n### Recursive descent\n\nRecursive descent parser is a program, where each [grammar](100-days/introduction-to-grammars) rule is implemented as a separate method.\nSometimes all rules describing one terminal are combined into a single method.\n\n{{\u003c svg \"static/recursive_parser.svg\" \u003e}}\n\n### Back-tracking and non back-tracking\n\nBack-tracking parser looks some tokens behind to define a position of current token in a tree. Non\nback-tracking parsers do not use previous tokens.\n\n# Most  common parser classification\n\nLL and LR are the most often mentioned parsers in this context. And their distinct features are\nbeing top-down and bottom-up respectfully.\n\n### Top-down parser.\n\nThis parser creates a tree from root to leaves.\n\n##### LL(k) parser\n\nLeft-to-right, leftmost derivation. Parses tokens from left to right, looking k\ntokens ahead. Uses context free grammar.\n\nCan be implemented either as a recursive descent or a pushdown automata.\n\n*Note: a subset of languages that can be parsed with LL(k) parser is\ncalled LL(k) languages.*\n\n### Bottom-up parser.\n\nUnlike top-down, creates tree starting from leaves and finishing at root.\n\n##### LR(k) parser\n\nLeft-to-right, rightmost derivation. Uses CFG. Starts with tokens of higher priority, eventually\nbuilding its way from leaves to root (the higher the node, the less priority it has. Look\nat [AST arithmetic example](100-days/ast)).\n\nImplementation uses pushdown automata or deterministic finite automation. Works in linear time.\n\n# Other parsers\n\n### Operator precedence parser\n\nIs a parser for, you guessed it, operator precedence grammars. OP is a CFG with no empty right-hand\nside rules and no rules with adjacent non-terminals.\n\n##### Operator precedence implementation\n\nPratt parser is a top-down operator precedence parser. This parser is rarely mentioned. It is based\non the idea of led, nud and std functions. Each token has (or has not) led, nud and std functions.\nThese functions basically tell how this token is related to ones around it. All parsing is embedded\ninto these functions, with some additional ones: expression and statement functions.\n\n* Nud is null denotation, used for primary tokens like numbers and\n  identifiers and prefix operators\n* Led is left denotation, describe how a token relates to its left neighbor. Used for infix\n* Std is statement denotation, is for statements: assignment, return keyword\n\n#### PEG parser\n\nParser parsing expression grammar.\n\n##### PEG parser implementation\n\nPackrat parser implements recursive descent with infinite lookahead. Requires more memory than LL\nparser, but supports left recursion.\n\n*I don't think this is an exhaustive list of parsers, and it's shallow. However, it's in sync with\ngrammar post, one can be read after another*","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/quality-of-life":{"title":"Quality of life","content":"\nRecently I started working on a new project, that also requires creating a custom programming\nlanguage. I noticed a significant difference with changing my approach: now I use python\nand [Lark](https://github.com/lark-parser/lark) for parsing EBNF.\n\n### Lark vs writing your own lexer and parser\n\nI regret not spending more time researching kotlin EBNF parsers. I tried ANTLR, but it had many\nincompatible versions, therefore I didn't use it. Additionally, antlr generated java code that\nwon't work with javascript. Plus, I found some others parsers, all of them generated parsers during\nruntime. I thought it was bad. However, Lark generates parsers during runtime too.\n\nWriting EBNF grammar is faster and less error-prone. Moreover, it is faster to change existing\ngrammar, compared to programmatically written lexer and parser. I\nrewrote [this](https://github.com/cristiandima/tdop) Go code into Kotlin and customized it for my\nlanguage. That took a lot of time, even without considering bug fixes.\n\n### Python vs Kotlin\n\nIt might not be as important for others, but I prefer interpreted languages because after a small\nchange in code it starts as fast, compared to compiled language. Compilation is a slow process that\nhinders development for me.\n\n### Working in a group vs alone\n\nThis time we work together with my friend, and I have to say, it is much better than working alone.\nWhen you are on your own, it is easy to miss some obvious mistakes. Discussing solutions leads to\nbetter ones. Maybe, that way it's not always going to be the way I want, but who said that my\nvision is correct? Besides, we both have a similar goal.\n\n### Conclusions\n\n1. Spend more time researching ready solutions. It will pay off during development, more time will\n   be saved overall\n2. If working alone, find likeminded people to share ideas and discuss design decisions. Get\n   feedback from them.\n3. ~~Use interpreted languages~~","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/shallow-villain-traits":{"title":"Shallow villain traits","content":"","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/transpilers-compilers-interpreters":{"title":"How languages are run?","content":"\nPeople in the programing languages field use fancy buzzwords like **translator, transpiler,\ncompiler, interpreter**, **JIT** (just in time) compiler. Let's figure out what they mean. But\nbefore let's create some sort of programming language hierarchy:\n\n1. High level programming language\n2. Assembly language (low-level language that is still programmable by humans)\n3. Machine code (runs on processor, no reason to write it, because it is undecipherable without\n   helper tools and is processor-specific)\n\n### Translator\n\n**Translator** is a general word for transpilers, compilers and interpreters. It's a tool\nconverting one code to another one (low-level or high level).\n\n### Compiler\n\n**Compilers** create machine code before runtime, which they save into binary files. Each\nprocessor[^1] requires a special compiler. Plus, compiler depends on a platform. Say, there is\na compiler for C++ targeted for the Linux OS and a particular processor.\n\nBenefits of compilers are:\n\n1. Run once to create executable files. Afterwards, run execs each time a program is started.\n2. Compilation is a form of program analysis, which will detect errors (called compilation errors)\n   before running a program, with, for instance, heavy calculations (thus, speeding the refactoring\n   process).\n\nBut there is a considerable drawback for a compiled language. Every little change in the program\nrequires recompilation of the whole module.\n\nLanguages that are compiled: C, C++, Haskell, Rust, Go\n\n### Interpreter\n\n**Interpreters**, on the other hand, can stumble upon such error that compiler could remove,\nbecause\nthey translate a program line by line, during runtime. Due to this translation thing going, a\ntranspiler is initially slower than an already compiled code. But there is a catch, when running a\nprogram line by line, there is more information that can be extracted, because interpreter will see\nvariable values that are impossible to analyse statically, before a program is run[^2]. That\nis where JIT compiler comes into play. It finds runtime optimizations and identifies parts of code\nwhich are faster to compile and execute rather than execute line by line (a default way of\ninterpreter).\n\nBriefly, interpreters:\n\n1. Translate code during runtime, line by line\n   (that is why most of the debuggers are interpreters).\n2. Do not create executable files.\n\nInterpreted languages: Python, Perl, Ruby\n\n### Transpiler\n\nIt is a tool that converts source code of one language to source code of the other language.\nUsually referred to as translators between two high level languages. Transpiler is called source to\nsource compiler sometimes.\n\n### Assembler\n\nAlso, there is a less frequently used term. **Assemblers**. Essentially, they are compilers for\nassembly language. Assemblers convert assembly code into machine code before runtime. I think it\nis a part of a compiler (compilation is a two-step process: translate language into assembly\nlanguage, then translate assembly into machine language).\n\n### Is Java compiled or interpreted?\n\nJava's source code is compiled into a binary byte code.\nThis code runs on JVM (technically not a\nprocessor), that usually is a blazingly fast interpreter (and\noften with a JIT compiler), that maps byte-code commands to processor\ncommands. Java can be considered both compiled and\ninterpreted language, similarly to Kotlin and Scala (JVM languages). Oftentimes though, Java is\nthought to be compiled.\n\nBut I think it is possible to write an interpreter for any compiled language and a compiler for any\ninterpreted language. Therefore, language classification in terms of compiled/interpreted\nlanguages is suggestive, not mandatory.\n\n### Linker\n\nLinker is a program that links compiled files. It resolves imports.\n\n[^1]: I'm not sure that compiler cannot target multiple processors at once. ISA (Instruction set\narchitecture) of processor is an important thing. Different processors with similar ISAs might be\ntargeted by one compiler.\n\n[^2]: And this cannot be fixed by compiling a program twice: first to run it and find optimizations\nduring runtime, and second to create optimized binary executables. This is because optimizations do\ndepend on the input arguments, which may vary vastly.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/100-days/why-gamemaking-is-not-for-me":{"title":"Why game making is not for me","content":"TLDR: it is frustrating, initial idea and result have a huge difference, because I associate my\ngame ideas with emotions, that a player feels during the game. When playing a made game I don't\nfeel these emotions at all.\n\n### Game feel\n\nGame feel is the set of emotions that a person experiences during play session. Game feel is\nsubjective for everyone. It is **the only thing that defines game value[^1]**.\n\n### How I used to make games\n\nI came up with a spectacular game idea[^2]. Not long time after, I imagine a default play session,\nwhat\na player should do, **how he would feel**. Right from the start I merge game feel with an idea. And\nthis is the path to failure.\n\nThing is, I'm a good engineer. I'm capable of implementing my game idea. But I also expect that\nwhen playing a resulting game, I would experience expected emotions. And it never happens.\nProbably, because it is a job of a game designer.\n\n{{\u003c svg \"static/gameidea_cycle.svg\" \u003e}}\n\n### Profession of a game designer\n\nI think game designer is responsible for the game feel. It's a whole field, that I know nothing\nabout. Unlike me, a good game designer can predict what emotions will a player feel during a game.\nOr, more importantly, game designer knows, **how** to change an existing product towards a better\ngame feel.\n\n### Conclusion\n\nTo prevent frustration, I shouldn't connect ideas with emotions. If I find an idea to be fun in my\nhead, I should not expect it to be fun in reality. There are two ways to be better at making fun\ngames:\n\n1. Learn some game design (not game creation)\n2. Iterate, hoping that you are heading towards better game feel. Might not work.\n\n[^1]: You might say: \"What a bold statement! How about beautiful graphics? Interesting mechanics?\nDon't you think that a game with a bad game feel is valuable because of its graphics?\" No, I don't.\nIn this case graphics are valuable by themselves. You evaluate graphics from the point of its\nvisual value. Therefore, if a game has a bad game feel, it's bad as a game, it's not fun to play.\nAll its parts might be fine but game value is not defined by them in separate, important thing is\n**how all pieces together contribute to the emotions that a player feels during the game**.\n\n[^2]: In this context, I refer to the **game idea** as an interesting original mechanic, raw\ngameplay loop, without much detail. I prototype this mechanic, because I think it would be fun to\nplay.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/Assignment":{"title":"","content":"# Assignment\nLeft side of the assignment before `=` is lvalue, right side is rvalue\n## Lvalue\nThere are 4 types of possible lvalues\n### Variable\n\n### Property\nSimilar to variables, however their scope is class or primitive.\n### Indexing\nChanging array's  element is possible with `arr[i] = ...`\n### Reference","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/notes/eyes":{"title":"Eye focus and main eye","content":"On sketches that illustrate eye focus, it is shown that humans, when looking straight ahead, focus their vision at a point between their eyes. I came to the conclusion that it is not true, and we focus similarly to the picture on the right.\n\nI have a small experimental proof for it. Look in the mirror. What point are you looking at most of the time? For me, it is the right eye. It is the 'main' eye. If you are looking at your eye too, try switching focus to a different eye and keep it there for some time. It might feel weird, and I think that's because you are switching your main eye. If you got acquainted with that feeling, try looking around with a switched main eye. It certainly feels as if you are focusing similarly to the right picture.\n\n{{\u003c svg \"static/eyes.svg\" \u003e}}   ","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/regina/dynamic-instantiation":{"title":"","content":"# Dynamic instantiation\n\nClasses can have references to other classes inside them as\nproperties. These property-classes are evaluated dynamically. Take a look at this example:\n\n```kotlin\nclass Node export rect {\n    // export = false - can deny export\n    iter = if (parent) parent.iter + 1 else 0\n\n    childNode = if (iter == 5) Nothing() else Node()\n\n    position = if (childNode is Node) \\\n    Position(childNode.position.x + 1, childNode.position.y) \\\n    else Position(0, 0)\n}\n\nfun main() {\n    Node()\n}\n```\n\nThis will create following svg:\n\n```svg\n\n\u003csvg\u003e\n    \u003crect x=\"4\" y=\"0\"/\u003e\n    \u003crect x=\"3\" y=\"0\"/\u003e\n    \u003crect x=\"2\" y=\"0\"/\u003e\n    \u003crect x=\"1\" y=\"0\"/\u003e\n    \u003crect x=\"0\" y=\"0\"/\u003e\n\u003c/svg\u003e\n```\n\nFirst algorithm creates empty Node() (we'll call it *Node0*) from main(). Then, starting from top to bottom:\n\n1. algorithm assigns **iter** of *Node0* to 0 because parent returns 0 as an equivalent of null.\n2. After that, **childNode** is assigned a new Node (*Node1*).\n3. **position** cannot be assigned because **childNode.position** is not yet assigned. Algorithm goes to\n   childNode.position, that is *Node1*.position and tries to assign it. However, childNode is required, so we go to *\n   Node1*.childNode, which needs iter.\n\n*It's not important to consider while assigning values, but it shows why there cannot be any cyclic dependencies for\nproperties*.\n\n## Implied decision\n\nAlso, it is the reason why class functions are impossible. Imagine this case:\n\n```kotlin\nclass FunctionOveruse {\n    prop = make()\n\n    fun make() {\n        FunctionOveruse()\n        return prop\n    }\n}\n```\n\nBoth lines in ```make()``` will execute forever.\n\nTo make functions as expressive as possible, it is important to allow class instantiating inside them. If\n\nWe either make internal class functions (which is purely decompositional thing) or make instantiating inside functions\npossible (and ```fun main()``` as an entry point)","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/regina/regina":{"title":"ReGIna","content":"\nis a [dynamically typed](notes/typization.md) language with [syntax](regina/syntax) similar to Kotlin and Python.\n\n## Core idea\n\nDifference between other languages and ReGIna is [dynamic instantiation](regina/dynamic-instantiation). In\nshort, properties in classes will be assigned only after their dependencies are assigned, allowing recursive properties\nof same class `A` inside class `A`.\n[plant](generators/plant.json)\n\n## Advantages\n\n* **Can be embedded into web**. Regina is written in Kotlin and compiled into Javascript.\n* **Rapid generator development**. Look at [examples](#Examples) to see what can be done and how much code is required.\n* **Customizable export**.\n\n## Drawbacks\n\n* **It's slow**. It's definitely slower than Python. Yet it's designed to be fast and concise in creating generators.\n    * **There is no community**. At least not yet. Mail me at kononal@gmail.com if you created something about Regina.\n\n## Examples\n","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/regina/syntax":{"title":"Syntax","content":"\nReGIna's syntax is an amalgamation of kotlin and python.\n\n## Declarations\n```python\nfrom typing import Iterator\n\n# This is an example\nclass Math:\n    @staticmethod\n    def fib(n: int) -\u003e Iterator[int]:\n        \"\"\" Fibonacci series up to n \"\"\"\n        a, b = 0, 1\n        while a \u003c n:\n            yield a\n            a, b = b, a + b\n\nresult = sum(Math.fib(42))\nprint(\"The answer is {}\".format(result))\n```\n### Class\n\nClass declaration should satisfy following format:\n\n```kotlin \nclass ClassName : SuperClassName export circle {\n    ...\n}\n```\n\nwhere `: SuperClassName` and `export ...` are optional.\n\n### Object\n\n```kotlin\nobject ObjectName export circle {\n    ...\n}\n```\n\nSimilar to type, although it cannot be inherited.\n\n### Function\n\n```kotlin\nfun functionName(arg0, arg1, ...) {\n    ...\n}\n```\n\n### Property/variable assignment\n\n```kotlin\nvariableName = ...\nclassName.PropertyName = ...\n```\n\nVariables and properties are dynamic, meaning they are type independent. `a` can be String and in the next line it can\nbe Int or class instance.\n\n## References\n\nReferences are expressions of form `a.b.c`. They serve 3 purposes:\n\n1. accessing class/object properties\n2. accessing primitive/class/object and functions\n3. specifying package declarations\n4. reducing expressions\n\n### 1. Accessing properties\n\n\n# Code conventions\n\nCamel case is used.\n\nNames of [class](Syntax.md/###Class), [object](Syntax.md/###Object) start with capital.\n\nVariables, functions start with lowercase letter.","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/regina/typization":{"title":"","content":"Dynamically typed languages check variable type during runtime, meaning it is more **error-prone**.On the contrary,\nstatically typed languages check variable type during compilation.\n\nOften dynamically typed languages can reassign the same variable with a different type. Also, variable declaration and\nassignment are similar for dynamic languages.\n\n```Python\n# Python\na = 2\na = \"2\" # not an error\n```\n\nOne of the clues of static typing is in variable declaration:\nusually variables have a type before a name in declaration:\n\n```C\n // C\n int a = 2;\n a = 3;\n```\n\nHowever, it is not always the case:\n\n```Kotlin\n// Kotlin\nval a = 2\n```\n\nIn Kotlin variable declaration prefix determines its mutability and type is derived from the expression on the right.\n","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null},"/regina/why":{"title":"","content":"# Why I created a programming language\n\nTLDR: I didn't think about making a library for dynamic instantiation. I should've made a library - that is an optimal path.\n\n## Birth of 'generator or generators'\n\nI liked creating visual generators. But I didn't like spending 2 weeks to flesh out an idea and to implement unuseful\ntools like deployment to web or geometry functions.\n\n*Looking back, I realise that I wasted much time due to switching between tools: for plant generator I used C# and .NET,\nfor House generator I used Godot and for map generator Kotlin with KorGE (there I spend much time figuring out how to\nuse KorGE and writing my own geometry function).*\n\nI needed a tool to create generators. And I started thinking about it. On this stage, there was no intention on creating\na language, all the more so, I was unqualified at that time. I unofficially called this tool the 'generator of\ngenerators'.\n\n## Generator stages\nI had these project specifications (from most to least priority):\n1. It should be possible to create generators of any complexity\n2. It should be concise\n3. Non-programmers should be able to use it\n### Web tool\n\nGeometry primitives:\n\n* Segment\n* Rectangle\n* Polygon/polyline (smooth)\n* Ellipse\n* Arc (circle)\n*\n\nThree types of containers:\n\n1. Variant\n2. Recursive\n3. Container\n\n![Image](images/firstdesign.jpg)\n\n### Early stages of language development\n\n### Upgrading language","lastmodified":"2022-09-03T13:24:38.011523466Z","tags":null}}